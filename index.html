<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>HMAW: Hierarchical Multi-Agent Workflow for Prompt Optimization</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">HMAW: Hierarchical Multi-Agent Workflow for Prompt Optimization</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=fBQdhG0AAAAJ&hl=en" target="_blank">Yuchi Liu</a>,</span>
                <span class="author-block">
                  <a href="https://1jsingh.github.io/" target="_blank">Jaskirat Singh</a>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=NIv_aeQAAAAJ&hl=en" target="_blank">Gaowen Liu</a>,</span>
                    <span class="author-block">
                      <a href="https://scholar.google.com/citations?user=9rHwD8wAAAAJ&hl=en" target="_blank">Ali Payani</a>,</span>
                      <span class="author-block">
                        <a href="https://scholar.google.com.au/citations?user=vNHqr3oAAAAJ&hl=en" target="_blank">Liang Zheng</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Australian National University, Cisco<br>ICLR 2025 Workshop on Reasoning and Planning for LLMs</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2405.20252" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/liuyvchi/HMAW" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2405.20252" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/prompting_compare.jpg" alt="MY ALT TEXT" />
  
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
      <!-- Your video here -->
      <!-- <source src="static/videos/banner_video.mp4" type="video/mp4" /> -->
      <!-- </video> -->
      <h2 class="subtitle has-text-centered">
        Examples comparing the generalization ability of existing methods and the proposed one. (a) COT uses a handcrafted prompt, which might not be suitable for all tasks.  (b) APE fine-tunes the prompt on a specific dataset, and its generalization capability to other scenarios is questionable. (c) ExperPrompting includes few-shot examples in the system prompt to help an LLM convert the user query to a format more suitable for LLM, but these examples might not be able to cover all scenarios. (d) Our method adopts a hierarchical design in reformatting the user query. Free from pre-defined few-shot examples, the interaction between the LLM hierarchy allows for more generalizable yet more adaptive tuning of the prompt.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) have shown great progress in responding to user questions, allowing for a multitude of diverse applications. Yet, the quality of LLM outputs heavily depends on the prompt design, where a good prompt might enable the LLM to answer a very challenging question correctly. Therefore, recent works developed many strategies for improving the prompt, including both manual crafting and in-domain optimization. However, their efficacy in unrestricted scenarios remains questionable, as the former depends on human design for specific questions and the latter usually generalizes poorly to unseen scenarios. To address these problems, we give LLMs the freedom to design the best prompts according to themselves. Specifically, we include a hierarchy of LLMs, first constructing a prompt with precise instructions and accurate wording in a hierarchical manner, and then using this prompt to generate the final answer to the user query. We term this pipeline Hierarchical Multi-Agent Workflow, or HMAW. In contrast with prior works, HMAW imposes no human restriction and requires no training, and is completely task-agnostic while capable of adjusting to the nuances of the underlying task. Through both quantitative and qualitative experiments across multiple benchmarks, we verify that despite its simplicity, the proposed approach can create detailed and suitable prompts, further boosting the performance of current LLMs. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Method Overview</h2>
         <p>
          <img src="static/images/method_overview.jpg" alt="MY ALT TEXT" />
            </p>
            <p>
              We propose modeling the prompt optimization problem as a zero-shot output within a multi-agent workflow.
              The initial query, <strong>q<sub>i</sub></strong>, is first inputted into the first layer of our framework (the COE layer).
              Before being processed by the CEO LLM agent, <strong>q<sub>i</sub></strong> is transformed into an LLM prompt
              <strong>p<sub>i</sub><sup>c</sup></strong> by the prompter <strong>f<sup>c</sup></strong>, which also concatenates it with the context
              <strong>C<sup>c</sup></strong> in the CEO layer. The output of the first layer, <strong>q<sub>i</sub><sup>c</sup></strong>, serves
              as the query from the CEO layer to the Manager layer.
              </p>
              
              <p>
              Similarly, the Manager Layer and the Worker Layer each include their own prompters, <strong>f<sup>m</sup></strong>
              and <strong>f<sup>w</sup></strong>, respectively. Besides concatenating the content of this layer, the initial
              query <strong>q<sub>i</sub></strong> is also concatenated to enhance stability. The input for the Worker LLM
              is our optimized prompt <strong>P<sub>i</sub><sup>*</sup></strong>, which directly triggers the LLM agent to
              generate the final response to the original query <strong>q<sub>i</sub></strong>.
              </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Results</h2>
         <p>
          <img src="static/images/result1.jpg" alt="MY ALT TEXT" />
            </p>
            <p class="has-text-centered">
              An example of prompt optimization using HMAW on the Education dataset. 
            </p>
            <p>
              <img src="static/images/result2.jpg" alt="MY ALT TEXT" />
                </p>
                <p class="has-text-centered">
                  A case study of HMAW on the CodeNet Dataset.
                </p>
                <p>
                  <img src="static/images/result3.jpg" alt="MY ALT TEXT" />
                    </p>
                    <p class="has-text-centered">
                      A case study of HMAW on the GSM8K Dataset. Colored texts indicate content coherence.
                    </p>
        </div>
      </div>
    </div>
  </div>
</section>











<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{liu2024hierarchical,
        title={Towards Hierarchical Multi-Agent Workflows for Zero-Shot Prompt Optimization}, 
        author={Yuchi Liu and Jaskirat Singh and Gaowen Liu and Ali Payani and Liang Zheng},
        year={2024},
        eprint={2405.20252},
        archivePrefix={arXiv},
        primaryClass={cs.CL}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
